{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models import ConvNet, MlleaksMLP,StudentNet\r\n",
    "import torch.optim as optim\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import numpy as np\r\n",
    "import torchvision\r\n",
    "from train_eval import train, eval_model, train_attacker, eval_attacker\r\n",
    "from custom_dataloader import dataloader\r\n",
    "import os\r\n",
    "import argparse\r\n",
    "from tqdm import tqdm\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchvision.transforms as transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('--dataset', default='cifar', help='The dataset of choice between \"cifar\" and \"mnist\".')\r\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='The batch size used for training.')\r\n",
    "parser.add_argument('--epoch', default=150, type=int, help='Number of epochs for shadow and target model.')\r\n",
    "parser.add_argument('--attack_epoch', default=50, type=int, help='Number of epochs for attack model.')\r\n",
    "parser.add_argument('--only_eval', default=False, type=bool, help='If true, only evaluate trained loaded models.')\r\n",
    "parser.add_argument('--save_new_models', default=False, type=bool, help='If true, trained models will be saved.')\r\n",
    "args = parser.parse_args(args=[])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = args.dataset\r\n",
    "args.save_new_models = True\r\n",
    "args.only_eval = False\r\n",
    "shadow_path, target_path, attack_path = \"./models/shadow_\" + str(dataset) + \".pth\", \\\r\n",
    "                                        \"./models/target_\" + str(dataset) + \".pth\", \\\r\n",
    "                                        \"./models/attack_\" + str(dataset) + \".pth\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if dataset == \"cifar\":\r\n",
    "    input_size = 3\r\n",
    "elif dataset == \"mnist\":\r\n",
    "    input_size = 1\r\n",
    "\r\n",
    "n_epochs = args.epoch\r\n",
    "attack_epochs = args.attack_epoch\r\n",
    "batch_size = args.batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shadow_train_loader = dataloader(dataset=dataset, batch_size_train=batch_size, batch_size_test=1000,\r\n",
    "                                     split_dataset=\"shadow_train\")\r\n",
    "shadow_out_loader = dataloader(dataset=dataset, batch_size_train=batch_size, batch_size_test=1000,\r\n",
    "                                   split_dataset=\"shadow_out\")\r\n",
    "target_train_loader = dataloader(dataset=dataset, batch_size_train=batch_size, batch_size_test=1000,\r\n",
    "                                     split_dataset=\"target_train\")\r\n",
    "target_out_loader = dataloader(dataset=dataset, batch_size_train=batch_size, batch_size_test=1000,\r\n",
    "                                   split_dataset=\"target_out\")\r\n",
    "\r\n",
    "testloader = dataloader(dataset=dataset, batch_size_train=batch_size, batch_size_test=1000,\r\n",
    "                            split_dataset=\"test\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_net = StudentNet()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shadow_net = StudentNet()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_net = target_net.to('cuda')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shadow_net = shadow_net.to('cuda')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_loss = shadow_loss = nn.CrossEntropyLoss()\r\n",
    "target_optim = optim.Adam(target_net.parameters(), lr=0.01)\r\n",
    "shadow_optim = optim.Adam(shadow_net.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model, data_loader, criterion, optimizer, verbose=True):\r\n",
    "    \"\"\"\r\n",
    "    Function for model training step\r\n",
    "    \"\"\"\r\n",
    "    running_loss = 0\r\n",
    "    model.train()\r\n",
    "    acc = 0\r\n",
    "    for step, (batch_img, batch_label) in enumerate(tqdm(data_loader)):\r\n",
    "        batch_img, batch_label = batch_img.to('cuda'), batch_label.to('cuda')\r\n",
    "        optimizer.zero_grad()  # Set gradients to zero\r\n",
    "        output = model(batch_img)  # Forward pass\r\n",
    "        loss = criterion(output, batch_label)\r\n",
    "        loss.backward()  # Backpropagation\r\n",
    "        optimizer.step()  # Update weights\r\n",
    "        running_loss += loss\r\n",
    "        yp = torch.max(output,dim=1)[1]\r\n",
    "        acc += (yp == batch_label).sum().item()\r\n",
    "        # Print loss for each minibatch\r\n",
    "    if verbose:\r\n",
    "        print(\"[%d/%d] loss = %f,acc = %f\" % (step, len(data_loader), loss.item(),acc/12500))\r\n",
    "    return running_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(20):\r\n",
    "    train(shadow_net,shadow_train_loader,shadow_loss, shadow_optim,verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(shadow_net.state_dict(),shadow_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eval_model(shadow_net, shadow_train_loader, report=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_net = torch.nn.DataParallel(target_net)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_net.load_state_dict(torch.load(\"./models/target_cifar1.pkl\"))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shadow_net.load_state_dict(torch.load(\"./models/shadow_cifar.pth\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eval_model(target_net, target_train_loader, report=True)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "attack_net = MlleaksMLP()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "attack_net = attack_net.to('cuda')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# Binary cross entropy as loss\r\n",
    "attack_loss = nn.BCELoss()\r\n",
    "attack_optim = optim.Adam(attack_net.parameters(), lr=0.005)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "attack_net.load_state_dict(torch.load(\"./models/attack_cifar.pkl\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "optimizer = attack_optim\r\n",
    "criterion = attack_loss\r\n",
    "# shadow_net = target_net\r\n",
    "shadow_net.eval()\r\n",
    "verbose = True\r\n",
    "total = 0\r\n",
    "correct = 0\r\n",
    "for i in range(20):\r\n",
    "    running_loss = 0\r\n",
    "    for step, ((train_img, _), (out_img, _)) in enumerate(tqdm(zip(shadow_train_loader, shadow_out_loader))):\r\n",
    "        # In case something is wrong with the dataloaders\r\n",
    "        train_img,out_img = train_img.to('cuda'),out_img.to('cuda')\r\n",
    "        if train_img.shape[0] != out_img.shape[0]:\r\n",
    "            break\r\n",
    "\r\n",
    "        minibatch_size = train_img.shape[0]\r\n",
    "\r\n",
    "        # Evaluate shadow train and out images on the shadow model to obtain the posterior probabilities\r\n",
    "        train_posteriors = F.softmax(shadow_net(train_img.detach()), dim=1)\r\n",
    "        out_posteriors = F.softmax(shadow_net(out_img.detach()), dim=1)\r\n",
    "\r\n",
    "        # Sort the train in and out posteriors in descending order, from high to low\r\n",
    "        train_sort, _ = torch.sort(train_posteriors, descending=True)\r\n",
    "        out_sort, _ = torch.sort(out_posteriors, descending=True)\r\n",
    "\r\n",
    "        # Here we keep the three maximal posteriors based on the paper\r\n",
    "        train_top_k = train_sort[:, :3].clone()\r\n",
    "        out_top_k = out_sort[:, :3].clone()\r\n",
    "\r\n",
    "        train_labels = torch.ones(minibatch_size)\r\n",
    "        out_labels = torch.zeros(minibatch_size)\r\n",
    "        train_labels,out_labels = train_labels.to('cuda'),out_labels.to('cuda')\r\n",
    "        optimizer.zero_grad()\r\n",
    "\r\n",
    "        # Forward pass\r\n",
    "        train_predictions = torch.squeeze(attack_net(train_top_k))\r\n",
    "        out_predictions = torch.squeeze(attack_net(out_top_k))\r\n",
    "\r\n",
    "        # The attacker uses the prediction of the shadow model on the whole shadow dataset(train and out). Thus two\r\n",
    "        # losses are computed and added\r\n",
    "        loss_train = criterion(train_predictions, train_labels)\r\n",
    "        loss_out = criterion(out_predictions, out_labels)\r\n",
    "\r\n",
    "        loss = (loss_train + loss_out) / 2\r\n",
    "        loss.backward()  # Backprop\r\n",
    "        optimizer.step()  # Update\r\n",
    "        running_loss += loss\r\n",
    "\r\n",
    "        \r\n",
    "        correct += (train_predictions >= 0.5).sum().item()\r\n",
    "        correct += (out_predictions < 0.5).sum().item()\r\n",
    "        total += train_predictions.size(0) + out_predictions.size(0)\r\n",
    "        accuracy = 100 * correct / total\r\n",
    "\r\n",
    "    print(\"[%d/%d] loss = %.2f, accuracy = %.2f\" % (i, len(shadow_train_loader), running_loss.item(), accuracy))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 24.63it/s]\n",
      "3it [00:00, 24.59it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0/196] loss = 87.78, accuracy = 78.88\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.29it/s]\n",
      "3it [00:00, 24.79it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1/196] loss = 87.64, accuracy = 78.92\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.43it/s]\n",
      "3it [00:00, 23.62it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2/196] loss = 87.47, accuracy = 78.97\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.43it/s]\n",
      "3it [00:00, 26.08it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3/196] loss = 87.19, accuracy = 79.02\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.62it/s]\n",
      "3it [00:00, 25.86it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4/196] loss = 87.17, accuracy = 79.02\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.36it/s]\n",
      "3it [00:00, 25.85it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5/196] loss = 87.28, accuracy = 79.04\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.29it/s]\n",
      "3it [00:00, 25.42it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6/196] loss = 87.37, accuracy = 79.04\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 24.64it/s]\n",
      "3it [00:00, 24.38it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7/196] loss = 87.49, accuracy = 79.02\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 24.79it/s]\n",
      "3it [00:00, 25.42it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[8/196] loss = 87.28, accuracy = 79.03\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.01it/s]\n",
      "3it [00:00, 25.63it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[9/196] loss = 87.80, accuracy = 79.03\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.66it/s]\n",
      "3it [00:00, 25.64it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10/196] loss = 87.23, accuracy = 79.04\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.40it/s]\n",
      "3it [00:00, 25.42it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[11/196] loss = 87.22, accuracy = 79.04\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.42it/s]\n",
      "3it [00:00, 25.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12/196] loss = 87.26, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.27it/s]\n",
      "3it [00:00, 25.42it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[13/196] loss = 87.12, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.39it/s]\n",
      "3it [00:00, 25.10it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[14/196] loss = 87.17, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.31it/s]\n",
      "3it [00:00, 25.00it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[15/196] loss = 87.21, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.37it/s]\n",
      "3it [00:00, 25.86it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[16/196] loss = 87.23, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.47it/s]\n",
      "3it [00:00, 25.42it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17/196] loss = 87.21, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.54it/s]\n",
      "3it [00:00, 25.42it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[18/196] loss = 87.09, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:07, 25.46it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[19/196] loss = 87.05, accuracy = 79.05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(attack_net.state_dict(),'./models/attack_cifar.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def eval_attacker(attack_model, target_model, target_train, target_out, num_posterior):\r\n",
    "    \"\"\"\r\n",
    "    Evaluate the accuracy, precision, and recall of attack model for in training set/out of the target's model training data.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "\r\n",
    "        target_model.eval()\r\n",
    "        attack_model.eval()\r\n",
    "\r\n",
    "        precisions = []\r\n",
    "        recalls = []\r\n",
    "        accuracies = []\r\n",
    "        acc = 0\r\n",
    "        thresholds = np.arange(0.01 ,0.8, 0.01)  # Give a range of thresholds from 0.50 to 0.80\r\n",
    "        total = np.zeros(len(thresholds))\r\n",
    "        correct = np.zeros(len(thresholds))\r\n",
    "        true_positives = np.zeros(len(thresholds))\r\n",
    "        false_positives = np.zeros(len(thresholds))\r\n",
    "        false_negatives = np.zeros(len(thresholds))\r\n",
    "        target_model = target_model.to('cuda')\r\n",
    "        attack_model = attack_model.to('cuda')\r\n",
    "        for step, ((train_img, _), (out_img, _)) in enumerate(tqdm(zip(target_train, target_out))):\r\n",
    "            train_img,out_img = train_img.to('cuda'),out_img.to('cuda')\r\n",
    "            # Compute posteriors for the samples in the target training set and out of the target training set.\r\n",
    "            train_posteriors = nn.Softmax(dim=1)(target_model(train_img.detach()))\r\n",
    "            out_posteriors = nn.Softmax(dim=1)(target_model(out_img.detach()))\r\n",
    "\r\n",
    "            # Sort them for high to low and pick top 3.\r\n",
    "            train_sort, _ = torch.sort(train_posteriors, descending=True)\r\n",
    "            train_top_k = train_sort[:, :num_posterior].clone()\r\n",
    "\r\n",
    "            out_sort, _ = torch.sort(out_posteriors, descending=True)\r\n",
    "            out_top_k = out_sort[:, :num_posterior].clone()\r\n",
    "\r\n",
    "            # Take the probabilities for top k most likely classes,\r\n",
    "            # Outputs closer to 1 belong in the training set or closer to 0, out of training set\r\n",
    "            train_predictions = torch.squeeze(attack_model(train_top_k))\r\n",
    "            out_predictions = torch.squeeze(attack_model(out_top_k))\r\n",
    "            acc += ((train_predictions>0.5).sum().item()+(out_predictions<0.5).sum().item())\r\n",
    "#             print(train_top_k,out_top_k)\r\n",
    "            # print(train_predictions,out_predictions)\r\n",
    "            # Evaluation of the attack model on the target dataset. The model is evaluated for different thresholds for\r\n",
    "            # the decision of the attack model to infer the membership\r\n",
    "            for i, t in enumerate(thresholds):\r\n",
    "                # True positive: attack model produces a prediction larger than the threshold for a training data member\r\n",
    "                true_positives[i] += (train_predictions >= t).sum().item()\r\n",
    "                # False positive: attack model produces a prediction larger than the threshold for a non member\r\n",
    "                false_positives[i] += (out_predictions >= t).sum().item()\r\n",
    "                # False negative: model predicts smaller than the threshold for a member of the training set.\r\n",
    "                false_negatives[i] += (train_predictions < t).sum().item()\r\n",
    "\r\n",
    "                correct[i] += (train_predictions >= t).sum().item()\r\n",
    "                correct[i] += (out_predictions < t).sum().item()\r\n",
    "                total[i] += train_predictions.size(0) + out_predictions.size(0)\r\n",
    "\r\n",
    "        # For all the thresholds print, accuracy, recall and precision of the attack model\r\n",
    "        for i, t in enumerate(thresholds):\r\n",
    "            accuracy = 100 * correct[i] / total[i]\r\n",
    "\r\n",
    "            # Check these conditions because they are on the denominator, to avoid dividing with 0\r\n",
    "            if true_positives[i] + false_positives[i] != 0:\r\n",
    "                precision = true_positives[i] / (true_positives[i] + false_positives[i])\r\n",
    "            else:\r\n",
    "                precision = 0\r\n",
    "            if true_positives[i] + false_negatives[i] != 0:\r\n",
    "                recall = true_positives[i] / (true_positives[i] + false_negatives[i])\r\n",
    "            else:\r\n",
    "                recall = 0\r\n",
    "            accuracies.append(accuracy)\r\n",
    "            precisions.append(precision)\r\n",
    "            recalls.append(recall)\r\n",
    "\r\n",
    "            print(\r\n",
    "                \"threshold = %.4f, accuracy = %.2f, precision = %.2f, recall = %.2f\" % (t, accuracy, precision, recall))\r\n",
    "\r\n",
    "    return max(accuracies)\r\n",
    "    # return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "max_accuracy = eval_attacker(attack_net, target_net, target_train_loader, target_out_loader, num_posterior=3)\r\n",
    "print(\"Attack model: epoch[%d/%d]   Accuracy on target set: %.5f\"\r\n",
    "        % (0 + 1, attack_epochs, max_accuracy))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:28,  6.98it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "threshold = 0.0100, accuracy = 68.58, precision = 0.66, recall = 0.75\n",
      "threshold = 0.0200, accuracy = 68.06, precision = 0.66, recall = 0.73\n",
      "threshold = 0.0300, accuracy = 67.67, precision = 0.66, recall = 0.72\n",
      "threshold = 0.0400, accuracy = 67.40, precision = 0.66, recall = 0.71\n",
      "threshold = 0.0500, accuracy = 67.19, precision = 0.66, recall = 0.70\n",
      "threshold = 0.0600, accuracy = 66.94, precision = 0.66, recall = 0.70\n",
      "threshold = 0.0700, accuracy = 66.75, precision = 0.66, recall = 0.69\n",
      "threshold = 0.0800, accuracy = 66.55, precision = 0.66, recall = 0.68\n",
      "threshold = 0.0900, accuracy = 66.35, precision = 0.66, recall = 0.68\n",
      "threshold = 0.1000, accuracy = 66.18, precision = 0.66, recall = 0.67\n",
      "threshold = 0.1100, accuracy = 66.02, precision = 0.66, recall = 0.67\n",
      "threshold = 0.1200, accuracy = 65.85, precision = 0.66, recall = 0.66\n",
      "threshold = 0.1300, accuracy = 65.68, precision = 0.66, recall = 0.66\n",
      "threshold = 0.1400, accuracy = 65.52, precision = 0.66, recall = 0.65\n",
      "threshold = 0.1500, accuracy = 65.38, precision = 0.66, recall = 0.65\n",
      "threshold = 0.1600, accuracy = 65.25, precision = 0.66, recall = 0.64\n",
      "threshold = 0.1700, accuracy = 65.07, precision = 0.65, recall = 0.64\n",
      "threshold = 0.1800, accuracy = 64.95, precision = 0.65, recall = 0.64\n",
      "threshold = 0.1900, accuracy = 64.80, precision = 0.65, recall = 0.63\n",
      "threshold = 0.2000, accuracy = 64.73, precision = 0.65, recall = 0.63\n",
      "threshold = 0.2100, accuracy = 64.62, precision = 0.65, recall = 0.62\n",
      "threshold = 0.2200, accuracy = 64.56, precision = 0.65, recall = 0.62\n",
      "threshold = 0.2300, accuracy = 64.45, precision = 0.65, recall = 0.62\n",
      "threshold = 0.2400, accuracy = 64.28, precision = 0.65, recall = 0.61\n",
      "threshold = 0.2500, accuracy = 64.13, precision = 0.65, recall = 0.61\n",
      "threshold = 0.2600, accuracy = 64.04, precision = 0.65, recall = 0.61\n",
      "threshold = 0.2700, accuracy = 63.92, precision = 0.65, recall = 0.60\n",
      "threshold = 0.2800, accuracy = 63.84, precision = 0.65, recall = 0.60\n",
      "threshold = 0.2900, accuracy = 63.68, precision = 0.65, recall = 0.60\n",
      "threshold = 0.3000, accuracy = 63.58, precision = 0.65, recall = 0.59\n",
      "threshold = 0.3100, accuracy = 63.49, precision = 0.65, recall = 0.59\n",
      "threshold = 0.3200, accuracy = 63.33, precision = 0.65, recall = 0.58\n",
      "threshold = 0.3300, accuracy = 63.28, precision = 0.65, recall = 0.58\n",
      "threshold = 0.3400, accuracy = 63.21, precision = 0.65, recall = 0.58\n",
      "threshold = 0.3500, accuracy = 63.06, precision = 0.65, recall = 0.58\n",
      "threshold = 0.3600, accuracy = 62.92, precision = 0.65, recall = 0.57\n",
      "threshold = 0.3700, accuracy = 62.82, precision = 0.65, recall = 0.57\n",
      "threshold = 0.3800, accuracy = 62.73, precision = 0.65, recall = 0.56\n",
      "threshold = 0.3900, accuracy = 62.64, precision = 0.65, recall = 0.56\n",
      "threshold = 0.4000, accuracy = 62.46, precision = 0.64, recall = 0.56\n",
      "threshold = 0.4100, accuracy = 62.32, precision = 0.64, recall = 0.55\n",
      "threshold = 0.4200, accuracy = 62.21, precision = 0.64, recall = 0.55\n",
      "threshold = 0.4300, accuracy = 62.08, precision = 0.64, recall = 0.54\n",
      "threshold = 0.4400, accuracy = 61.90, precision = 0.64, recall = 0.54\n",
      "threshold = 0.4500, accuracy = 61.74, precision = 0.64, recall = 0.53\n",
      "threshold = 0.4600, accuracy = 61.63, precision = 0.64, recall = 0.53\n",
      "threshold = 0.4700, accuracy = 61.49, precision = 0.64, recall = 0.53\n",
      "threshold = 0.4800, accuracy = 61.36, precision = 0.64, recall = 0.52\n",
      "threshold = 0.4900, accuracy = 61.18, precision = 0.64, recall = 0.52\n",
      "threshold = 0.5000, accuracy = 61.08, precision = 0.64, recall = 0.51\n",
      "threshold = 0.5100, accuracy = 60.86, precision = 0.64, recall = 0.51\n",
      "threshold = 0.5200, accuracy = 60.72, precision = 0.64, recall = 0.50\n",
      "threshold = 0.5300, accuracy = 60.47, precision = 0.63, recall = 0.49\n",
      "threshold = 0.5400, accuracy = 60.21, precision = 0.63, recall = 0.49\n",
      "threshold = 0.5500, accuracy = 59.97, precision = 0.63, recall = 0.48\n",
      "threshold = 0.5600, accuracy = 59.78, precision = 0.63, recall = 0.47\n",
      "threshold = 0.5700, accuracy = 59.59, precision = 0.63, recall = 0.47\n",
      "threshold = 0.5800, accuracy = 59.40, precision = 0.63, recall = 0.46\n",
      "threshold = 0.5900, accuracy = 59.17, precision = 0.63, recall = 0.46\n",
      "threshold = 0.6000, accuracy = 59.00, precision = 0.63, recall = 0.45\n",
      "threshold = 0.6100, accuracy = 58.76, precision = 0.62, recall = 0.44\n",
      "threshold = 0.6200, accuracy = 58.48, precision = 0.62, recall = 0.43\n",
      "threshold = 0.6300, accuracy = 58.18, precision = 0.62, recall = 0.43\n",
      "threshold = 0.6400, accuracy = 57.87, precision = 0.62, recall = 0.42\n",
      "threshold = 0.6500, accuracy = 57.55, precision = 0.61, recall = 0.41\n",
      "threshold = 0.6600, accuracy = 57.21, precision = 0.61, recall = 0.40\n",
      "threshold = 0.6700, accuracy = 56.90, precision = 0.61, recall = 0.39\n",
      "threshold = 0.6800, accuracy = 56.54, precision = 0.61, recall = 0.37\n",
      "threshold = 0.6900, accuracy = 56.25, precision = 0.60, recall = 0.36\n",
      "threshold = 0.7000, accuracy = 55.88, precision = 0.60, recall = 0.35\n",
      "threshold = 0.7100, accuracy = 55.38, precision = 0.60, recall = 0.33\n",
      "threshold = 0.7200, accuracy = 54.78, precision = 0.59, recall = 0.31\n",
      "threshold = 0.7300, accuracy = 54.19, precision = 0.59, recall = 0.29\n",
      "threshold = 0.7400, accuracy = 53.62, precision = 0.58, recall = 0.26\n",
      "threshold = 0.7500, accuracy = 52.78, precision = 0.57, recall = 0.23\n",
      "threshold = 0.7600, accuracy = 51.40, precision = 0.55, recall = 0.16\n",
      "threshold = 0.7700, accuracy = 50.00, precision = 0.00, recall = 0.00\n",
      "threshold = 0.7800, accuracy = 50.00, precision = 0.00, recall = 0.00\n",
      "threshold = 0.7900, accuracy = 50.00, precision = 0.00, recall = 0.00\n",
      "Attack model: epoch[1/50]   Accuracy on target set: 68.58400\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "transform = transforms.Compose(\r\n",
    "                [transforms.ToTensor(),\r\n",
    "                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\r\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',train = True,download=True,transform=transform)\r\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=1,shuffle=True,num_workers=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "trainset = torch.load('./data/mode_0/trainset(all')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def divide(attack_model,target_model,dataloader,num_posterior):\r\n",
    "    attack_model = attack_model.to('cuda')\r\n",
    "    target_model = target_model.to('cuda')\r\n",
    "    target_model.eval()\r\n",
    "    attack_model.eval()\r\n",
    "    correct = 0\r\n",
    "    total = 0\r\n",
    "    \r\n",
    "    thresholds = np.arange(0.01, 0.6, 0.01)\r\n",
    "    trainset = []\r\n",
    "    for i in range(len(thresholds)):\r\n",
    "        x = []\r\n",
    "        trainset.append(x)\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for step,(train_img,train_labels) in enumerate(tqdm(dataloader)):\r\n",
    "            train_img = train_img.to('cuda')\r\n",
    "            train_posteriors = nn.Softmax(dim=1)(target_model(train_img.detach()))\r\n",
    "            train_sort, _ = torch.sort(train_posteriors, descending=True)\r\n",
    "            train_top_k = train_sort[:, :num_posterior].clone()\r\n",
    "            train_predictions = torch.squeeze(attack_model(train_top_k))\r\n",
    "#             print(train_predictions)\r\n",
    "            for i, t in enumerate(thresholds):\r\n",
    "                if train_predictions > t:\r\n",
    "                    train_img = train_img.squeeze()\r\n",
    "                    train_labels = train_labels.squeeze()\r\n",
    "                    traindata = (train_img,train_labels)\r\n",
    "                \r\n",
    "                    trainset[i].append(traindata)\r\n",
    "            # if train_predictions > 0.57:\r\n",
    "            #     train_img = train_img.squeeze()\r\n",
    "            #     trainset.append(train_img)\r\n",
    "            \r\n",
    "                \r\n",
    "    return trainset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "trainset = divide(attack_net,target_net,trainloader,num_posterior=3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50000/50000 [14:36<00:00, 57.06it/s]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "0.5 32152 \r\n",
    "0.6 28490"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "for i,t in enumerate(trainset):\r\n",
    "    print(0.01*i+0.01,len(t))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.01 23574\n",
      "0.02 23039\n",
      "0.03 22721\n",
      "0.04 22449\n",
      "0.05 22225\n",
      "0.060000000000000005 22030\n",
      "0.06999999999999999 21866\n",
      "0.08 21684\n",
      "0.09 21519\n",
      "0.09999999999999999 21373\n",
      "0.11 21220\n",
      "0.12 21069\n",
      "0.13 20955\n",
      "0.14 20847\n",
      "0.15000000000000002 20739\n",
      "0.16 20642\n",
      "0.17 20525\n",
      "0.18000000000000002 20424\n",
      "0.19 20314\n",
      "0.2 20224\n",
      "0.21000000000000002 20133\n",
      "0.22 20041\n",
      "0.23 19929\n",
      "0.24000000000000002 19831\n",
      "0.25 19731\n",
      "0.26 19657\n",
      "0.27 19559\n",
      "0.28 19463\n",
      "0.29000000000000004 19366\n",
      "0.3 19276\n",
      "0.31 19174\n",
      "0.32 19070\n",
      "0.33 18988\n",
      "0.34 18905\n",
      "0.35000000000000003 18816\n",
      "0.36000000000000004 18717\n",
      "0.37 18627\n",
      "0.38 18533\n",
      "0.39 18437\n",
      "0.4 18330\n",
      "0.41000000000000003 18219\n",
      "0.42000000000000004 18110\n",
      "0.43 17995\n",
      "0.44 17882\n",
      "0.45 17771\n",
      "0.46 17668\n",
      "0.47000000000000003 17537\n",
      "0.48000000000000004 17417\n",
      "0.49 17279\n",
      "0.5 17151\n",
      "0.51 17009\n",
      "0.52 16876\n",
      "0.53 16702\n",
      "0.54 16530\n",
      "0.55 16377\n",
      "0.56 16211\n",
      "0.5700000000000001 16051\n",
      "0.5800000000000001 15877\n",
      "0.59 15703\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "trainset21 = trainset[21]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "len(trainset21)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20041"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "torch.save(trainset21,'./data/mode_0/trainset20041')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "torch.save(trainset,'./data/mode_0/trainset(all')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def huan(dataloader):\r\n",
    "    trainset = []\r\n",
    "    for i,(x,_) in enumerate(dataloader):\r\n",
    "        x = x.squeeze(dim=0)\r\n",
    "        trainset.append(x)\r\n",
    "    return trainset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "targetloader = dataloader(dataset=dataset, batch_size_train=1, batch_size_test=1000,\r\n",
    "                                     split_dataset=\"target_train\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "targetset = huan(targetloader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "len(targetset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "traindata = torch.Tensor()\r\n",
    "type(traindata)\r\n",
    "x = torch.Tensor([5])\r\n",
    "for i in range(5):\r\n",
    "    traindata = torch.cat((traindata,x),dim=0)\r\n",
    "traindata"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([5., 5., 5., 5., 5.])"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "def judge():\r\n",
    "    acc = 0\r\n",
    "    traindata = torch.Tensor()\r\n",
    "    trainlabel = torch.Tensor()\r\n",
    "    for s,(y,b) in enumerate(tqdm(trainset21)):\r\n",
    "        for x in targetset:\r\n",
    "            x = x.to('cpu')\r\n",
    "            y = y.to('cpu')\r\n",
    "            if torch.equal(x,y):\r\n",
    "                acc += 1\r\n",
    "                y = y.unsqueeze(0)\r\n",
    "                traindata = torch.cat((traindata,y),dim=0)\r\n",
    "                trainlabel = torch.cat((trainlabel,b))\r\n",
    "                break\r\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "acc,traindata,trainlabel = judge()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/20041 [00:00<18:22, 18.18it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 1) cannot be concatenated",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-4421b2511ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjudge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-b5e46169ec82>\u001b[0m in \u001b[0;36mjudge\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mtraindata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0mtrainlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 1) cannot be concatenated"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "acc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7777"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "traindata[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[-1.1798, -1.1321, -1.1004,  ..., -1.6719, -1.6878, -1.7037],\n",
       "          [-1.3544, -1.3226, -1.2909,  ..., -1.3068, -1.2909, -1.3226],\n",
       "          [-1.4338, -1.4338, -1.4497,  ..., -1.0845, -1.0527, -1.1163],\n",
       "          ...,\n",
       "          [ 1.5511,  1.5034,  1.5034,  ...,  1.8845,  1.7733,  1.7416],\n",
       "          [ 1.0906,  1.0589,  1.0747,  ...,  1.2653,  1.2653,  1.2335],\n",
       "          [ 1.2653,  1.2494,  1.3288,  ...,  1.5669,  1.4558,  1.1700]],\n",
       " \n",
       "         [[-1.3873, -1.3711, -1.3550,  ..., -1.6939, -1.6939, -1.7100],\n",
       "          [-1.6616, -1.6132, -1.5648,  ..., -1.4679, -1.4679, -1.4841],\n",
       "          [-1.7584, -1.7262, -1.7100,  ..., -1.4195, -1.3873, -1.4195],\n",
       "          ...,\n",
       "          [ 1.2755,  1.2433,  1.2433,  ...,  1.7435,  1.6306,  1.5822],\n",
       "          [ 0.7753,  0.7430,  0.7591,  ...,  1.0657,  1.0819,  1.0335],\n",
       "          [ 0.9205,  0.9044,  0.9851,  ...,  1.3078,  1.1949,  0.9205]],\n",
       " \n",
       "         [[-1.4102, -1.3802, -1.3651,  ..., -1.4703, -1.4854, -1.4854],\n",
       "          [-1.5755, -1.5304, -1.5004,  ..., -1.3501, -1.3351, -1.3651],\n",
       "          [-1.5905, -1.5755, -1.5755,  ..., -1.3501, -1.3201, -1.3802],\n",
       "          ...,\n",
       "          [ 0.9187,  0.8886,  0.8736,  ...,  1.3093,  1.2042,  1.1741],\n",
       "          [ 0.4379,  0.4078,  0.4228,  ...,  0.6783,  0.6933,  0.6632],\n",
       "          [ 0.5581,  0.5430,  0.6182,  ...,  0.9037,  0.7985,  0.5280]]]),\n",
       " tensor(7))"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "15698  5667\r\n",
    "21543  8442"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "torch.save(traindata,'./data/mode_0/trainset(7777-20041)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mode_0 trainset  0.57 21890  8604"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "0.21 20041 7777\r\n",
    "0.58 15703 5696"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('yinwenxuan': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "interpreter": {
   "hash": "bcd566906ce8887e8b2edc749b60ff2b4389e49ce74441c35125aff3497da109"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}